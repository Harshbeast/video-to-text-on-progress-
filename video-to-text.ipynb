{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T19:29:12.885642Z",
     "iopub.status.busy": "2025-09-05T19:29:12.884527Z",
     "iopub.status.idle": "2025-09-05T19:29:12.897140Z",
     "shell.execute_reply": "2025-09-05T19:29:12.895477Z",
     "shell.execute_reply.started": "2025-09-05T19:29:12.885607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"/kaggle/input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T19:29:42.196114Z",
     "iopub.status.busy": "2025-09-05T19:29:42.195651Z",
     "iopub.status.idle": "2025-09-05T19:29:42.205157Z",
     "shell.execute_reply": "2025-09-05T19:29:42.204059Z",
     "shell.execute_reply.started": "2025-09-05T19:29:42.196087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(os.listdir(\"/kaggle/input/dataforvtt/data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T19:32:57.234759Z",
     "iopub.status.busy": "2025-09-05T19:32:57.234580Z",
     "iopub.status.idle": "2025-09-05T19:32:57.252694Z",
     "shell.execute_reply": "2025-09-05T19:32:57.252003Z",
     "shell.execute_reply.started": "2025-09-05T19:32:57.234741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = \"/kaggle/input/data-val/data_val\"\n",
    "print(os.listdir(data_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T19:37:27.518055Z",
     "iopub.status.busy": "2025-11-13T19:37:27.517744Z",
     "iopub.status.idle": "2025-11-13T21:46:10.714196Z",
     "shell.execute_reply": "2025-11-13T21:46:10.713533Z",
     "shell.execute_reply.started": "2025-11-13T19:37:27.518026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n",
      "100%|██████████| 230M/230M [00:01<00:00, 203MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Step 10, Train Loss 9.8233\n",
      "Epoch 1/5, Step 20, Train Loss 8.6549\n",
      "Epoch 1/5, Step 30, Train Loss 8.4684\n",
      "Epoch 1/5, Step 40, Train Loss 7.3511\n",
      "Epoch 1/5, Step 50, Train Loss 6.8163\n",
      "Epoch 1/5, Step 60, Train Loss 6.4702\n",
      "Epoch 1/5, Step 70, Train Loss 6.3055\n",
      "Epoch 1/5, Step 80, Train Loss 5.7503\n",
      "Epoch 1/5, Step 90, Train Loss 6.7666\n",
      "Epoch 1/5, Step 100, Train Loss 6.5504\n",
      "Epoch 1/5, Step 110, Train Loss 5.7121\n",
      "Epoch 1/5, Step 120, Train Loss 6.3460\n",
      "Epoch 1/5, Step 130, Train Loss 5.6124\n",
      "Epoch 1/5, Step 140, Train Loss 5.3321\n",
      "Epoch 1/5, Step 150, Train Loss 6.6381\n",
      "Epoch 1/5, Step 160, Train Loss 6.6829\n",
      "Epoch 1/5, Step 170, Train Loss 5.9128\n",
      "Epoch 1/5, Step 180, Train Loss 5.0446\n",
      "Epoch 1/5, Step 190, Train Loss 4.5552\n",
      "Epoch 1/5, Step 200, Train Loss 5.6204\n",
      "Epoch 1/5, Step 210, Train Loss 5.9888\n",
      "Epoch 1/5, Step 220, Train Loss 5.7405\n",
      "Epoch 1/5, Step 230, Train Loss 5.3708\n",
      "Epoch 1/5, Step 240, Train Loss 5.7389\n",
      "Epoch 1/5, Step 250, Train Loss 6.6341\n",
      "Epoch 1/5, Step 260, Train Loss 5.5549\n",
      "Epoch 1/5, Step 270, Train Loss 5.5641\n",
      "Epoch 1/5, Step 280, Train Loss 5.2239\n",
      "Epoch 1/5, Step 290, Train Loss 5.3510\n",
      "Epoch 1/5, Step 300, Train Loss 5.7430\n",
      "Epoch 1/5, Step 310, Train Loss 4.6677\n",
      "Epoch 1/5, Step 320, Train Loss 5.3215\n",
      "Epoch 1/5, Step 330, Train Loss 4.6149\n",
      "Epoch 1/5, Step 340, Train Loss 5.5417\n",
      "Epoch 1/5, Step 350, Train Loss 5.8015\n",
      "Epoch 1/5, Step 360, Train Loss 4.8485\n",
      "Epoch 1/5, Step 370, Train Loss 6.5966\n",
      "Epoch 1/5, Step 380, Train Loss 5.3499\n",
      "Epoch 1/5, Step 390, Train Loss 5.2668\n",
      "Epoch 1/5, Step 400, Train Loss 4.9494\n",
      "Epoch 1/5, Step 410, Train Loss 5.6659\n",
      "Epoch 1/5, Step 420, Train Loss 5.0737\n",
      "Epoch 1/5, Step 430, Train Loss 4.9436\n",
      "Epoch 1/5, Step 440, Train Loss 4.3569\n",
      "Epoch 1/5, Step 450, Train Loss 5.0305\n",
      "Epoch 1/5, Step 460, Train Loss 5.9684\n",
      "Epoch 1/5, Step 470, Train Loss 5.1997\n",
      "Epoch 1/5, Step 480, Train Loss 5.3981\n",
      "Epoch 1/5, Step 490, Train Loss 4.4375\n",
      "Epoch 1/5, Step 500, Train Loss 5.4112\n",
      "Epoch 1/5, Step 510, Train Loss 4.3489\n",
      "Epoch 1/5, Step 520, Train Loss 5.1822\n",
      "Epoch 1/5, Step 530, Train Loss 4.9184\n",
      "Epoch 1/5, Step 540, Train Loss 4.2506\n",
      "Epoch 1/5, Step 550, Train Loss 4.6689\n",
      "Epoch 1/5, Step 560, Train Loss 5.5449\n",
      "Epoch 1/5, Step 570, Train Loss 4.7529\n",
      "Epoch 1/5, Step 580, Train Loss 4.4415\n",
      "Epoch 1/5, Step 590, Train Loss 5.4270\n",
      "Epoch 1/5, Step 600, Train Loss 4.2191\n",
      "Epoch 1/5, Step 610, Train Loss 5.0558\n",
      "Epoch 1/5, Step 620, Train Loss 4.7750\n",
      "Epoch 1/5, Step 630, Train Loss 5.2884\n",
      "Epoch 1/5, Step 640, Train Loss 5.4972\n",
      "Epoch 1/5, Step 650, Train Loss 5.1930\n",
      "Epoch 1/5, Step 660, Train Loss 4.3609\n",
      "Epoch 1/5, Step 670, Train Loss 5.3352\n",
      "Epoch 1/5, Step 680, Train Loss 5.1691\n",
      "Epoch 1/5, Step 690, Train Loss 4.7023\n",
      "Epoch 1/5, Step 700, Train Loss 6.0605\n",
      "Epoch 1/5, Step 710, Train Loss 5.8310\n",
      "Epoch 1/5, Step 720, Train Loss 5.0125\n",
      "Epoch 1/5, Step 730, Train Loss 4.7612\n",
      "Epoch 1/5, Step 740, Train Loss 4.7664\n",
      "Epoch 1/5, Step 750, Train Loss 4.8681\n",
      "Epoch 1/5, Step 760, Train Loss 5.1697\n",
      "Epoch 1/5, Step 770, Train Loss 4.4872\n",
      "Epoch 1/5, Step 780, Train Loss 5.7880\n",
      "Epoch 1/5, Step 790, Train Loss 4.1360\n",
      "Epoch 1/5, Step 800, Train Loss 4.6210\n",
      "Epoch 1/5, Step 810, Train Loss 4.9608\n",
      "Epoch 1/5, Step 820, Train Loss 5.4460\n",
      "Epoch 1/5, Step 830, Train Loss 5.0978\n",
      "Epoch 1/5, Step 840, Train Loss 4.6820\n",
      "Epoch 1/5, Step 850, Train Loss 5.4402\n",
      "Epoch 1/5, Step 860, Train Loss 4.9634\n",
      "Epoch 1/5, Step 870, Train Loss 5.0081\n",
      "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
      "Progress: 384.5M / 384.5M (100.0%)\n",
      "Extracting stanford-corenlp-3.6.0 ...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/usr/local/lib/python3.11/dist-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [37.372 seconds]\n",
      "Parsing test captions\n",
      "Threads( StanfordCoreNLP ) Nov 13, 2025 8:03:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:19 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:23 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:28 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:29 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:39 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:40 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:41 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:42 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:46 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:47 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:48 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:54 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:03:59 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:05 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:07 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:15 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u001e (U+1E, decimal: 30)\n",
      "Nov 13, 2025 8:04:18 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:22 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:22 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:22 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:28 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:36 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:38 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:41 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:45 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:49 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:51 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:55 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:58 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:59 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:04:59 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:05 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:14 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:29 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:33 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:41 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:05:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "[02:46.183 minutes]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 3.543 min\n",
      "Epoch 1 DONE. Val Loss 4.8524, Val Acc 30.23%, SPICE 0.0631\n",
      "Epoch 2/5, Step 10, Train Loss 4.5143\n",
      "Epoch 2/5, Step 20, Train Loss 5.0056\n",
      "Epoch 2/5, Step 30, Train Loss 5.9333\n",
      "Epoch 2/5, Step 40, Train Loss 4.5362\n",
      "Epoch 2/5, Step 50, Train Loss 3.9123\n",
      "Epoch 2/5, Step 60, Train Loss 4.3847\n",
      "Epoch 2/5, Step 70, Train Loss 4.8134\n",
      "Epoch 2/5, Step 80, Train Loss 5.2461\n",
      "Epoch 2/5, Step 90, Train Loss 4.9668\n",
      "Epoch 2/5, Step 100, Train Loss 3.8438\n",
      "Epoch 2/5, Step 110, Train Loss 3.8587\n",
      "Epoch 2/5, Step 120, Train Loss 4.4336\n",
      "Epoch 2/5, Step 130, Train Loss 4.6415\n",
      "Epoch 2/5, Step 140, Train Loss 4.0564\n",
      "Epoch 2/5, Step 150, Train Loss 4.0636\n",
      "Epoch 2/5, Step 160, Train Loss 4.8708\n",
      "Epoch 2/5, Step 170, Train Loss 4.5541\n",
      "Epoch 2/5, Step 180, Train Loss 4.9501\n",
      "Epoch 2/5, Step 190, Train Loss 4.6782\n",
      "Epoch 2/5, Step 200, Train Loss 4.4651\n",
      "Epoch 2/5, Step 210, Train Loss 4.4435\n",
      "Epoch 2/5, Step 220, Train Loss 5.1831\n",
      "Epoch 2/5, Step 230, Train Loss 4.2720\n",
      "Epoch 2/5, Step 240, Train Loss 4.3881\n",
      "Epoch 2/5, Step 250, Train Loss 4.1427\n",
      "Epoch 2/5, Step 260, Train Loss 4.2819\n",
      "Epoch 2/5, Step 270, Train Loss 4.4254\n",
      "Epoch 2/5, Step 280, Train Loss 4.2061\n",
      "Epoch 2/5, Step 290, Train Loss 4.3347\n",
      "Epoch 2/5, Step 300, Train Loss 4.9180\n",
      "Epoch 2/5, Step 310, Train Loss 3.8216\n",
      "Epoch 2/5, Step 320, Train Loss 4.5199\n",
      "Epoch 2/5, Step 330, Train Loss 4.7373\n",
      "Epoch 2/5, Step 340, Train Loss 4.3242\n",
      "Epoch 2/5, Step 350, Train Loss 5.1112\n",
      "Epoch 2/5, Step 360, Train Loss 3.8931\n",
      "Epoch 2/5, Step 370, Train Loss 4.6761\n",
      "Epoch 2/5, Step 380, Train Loss 4.0237\n",
      "Epoch 2/5, Step 390, Train Loss 4.7247\n",
      "Epoch 2/5, Step 400, Train Loss 4.4163\n",
      "Epoch 2/5, Step 410, Train Loss 4.8949\n",
      "Epoch 2/5, Step 420, Train Loss 4.5694\n",
      "Epoch 2/5, Step 430, Train Loss 4.3451\n",
      "Epoch 2/5, Step 440, Train Loss 3.8574\n",
      "Epoch 2/5, Step 450, Train Loss 4.4031\n",
      "Epoch 2/5, Step 460, Train Loss 4.2899\n",
      "Epoch 2/5, Step 470, Train Loss 5.5804\n",
      "Epoch 2/5, Step 480, Train Loss 4.9183\n",
      "Epoch 2/5, Step 490, Train Loss 4.3319\n",
      "Epoch 2/5, Step 500, Train Loss 4.3875\n",
      "Epoch 2/5, Step 510, Train Loss 4.9828\n",
      "Epoch 2/5, Step 520, Train Loss 4.4901\n",
      "Epoch 2/5, Step 530, Train Loss 4.5408\n",
      "Epoch 2/5, Step 540, Train Loss 4.0707\n",
      "Epoch 2/5, Step 550, Train Loss 4.5590\n",
      "Epoch 2/5, Step 560, Train Loss 4.1446\n",
      "Epoch 2/5, Step 570, Train Loss 4.6259\n",
      "Epoch 2/5, Step 580, Train Loss 3.7482\n",
      "Epoch 2/5, Step 590, Train Loss 4.8400\n",
      "Epoch 2/5, Step 600, Train Loss 4.1977\n",
      "Epoch 2/5, Step 610, Train Loss 5.0836\n",
      "Epoch 2/5, Step 620, Train Loss 4.1185\n",
      "Epoch 2/5, Step 630, Train Loss 4.5713\n",
      "Epoch 2/5, Step 640, Train Loss 4.5770\n",
      "Epoch 2/5, Step 650, Train Loss 4.3467\n",
      "Epoch 2/5, Step 660, Train Loss 4.9134\n",
      "Epoch 2/5, Step 670, Train Loss 4.4214\n",
      "Epoch 2/5, Step 680, Train Loss 4.0514\n",
      "Epoch 2/5, Step 690, Train Loss 4.5990\n",
      "Epoch 2/5, Step 700, Train Loss 3.9835\n",
      "Epoch 2/5, Step 710, Train Loss 4.1126\n",
      "Epoch 2/5, Step 720, Train Loss 4.9277\n",
      "Epoch 2/5, Step 730, Train Loss 3.5896\n",
      "Epoch 2/5, Step 740, Train Loss 4.2850\n",
      "Epoch 2/5, Step 750, Train Loss 4.3787\n",
      "Epoch 2/5, Step 760, Train Loss 5.6844\n",
      "Epoch 2/5, Step 770, Train Loss 3.5650\n",
      "Epoch 2/5, Step 780, Train Loss 4.5816\n",
      "Epoch 2/5, Step 790, Train Loss 3.9800\n",
      "Epoch 2/5, Step 800, Train Loss 3.7263\n",
      "Epoch 2/5, Step 810, Train Loss 3.9127\n",
      "Epoch 2/5, Step 820, Train Loss 4.6469\n",
      "Epoch 2/5, Step 830, Train Loss 4.8568\n",
      "Epoch 2/5, Step 840, Train Loss 4.2416\n",
      "Epoch 2/5, Step 850, Train Loss 4.7745\n",
      "Epoch 2/5, Step 860, Train Loss 4.5621\n",
      "Epoch 2/5, Step 870, Train Loss 4.2847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/usr/local/lib/python3.11/dist-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) Nov 13, 2025 8:27:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:27:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0010 (U+10, decimal: 16)\n",
      "Nov 13, 2025 8:27:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:27:38 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:27:47 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:00 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:05 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:06 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0010 (U+10, decimal: 16)\n",
      "Nov 13, 2025 8:28:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:19 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:26 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:34 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:36 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:53 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:55 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:28:59 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:01 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:12 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:13 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:29 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:39 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:47 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:54 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:29:54 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:08 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:22 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:27 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:36 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:30:43 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:31:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "[03:47.484 minutes]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 3.950 min\n",
      "Epoch 2 DONE. Val Loss 4.5730, Val Acc 32.44%, SPICE 0.0795\n",
      "Epoch 3/5, Step 10, Train Loss 3.8931\n",
      "Epoch 3/5, Step 20, Train Loss 2.9807\n",
      "Epoch 3/5, Step 30, Train Loss 4.5647\n",
      "Epoch 3/5, Step 40, Train Loss 3.3538\n",
      "Epoch 3/5, Step 50, Train Loss 4.2478\n",
      "Epoch 3/5, Step 60, Train Loss 4.6744\n",
      "Epoch 3/5, Step 70, Train Loss 4.4727\n",
      "Epoch 3/5, Step 80, Train Loss 4.4343\n",
      "Epoch 3/5, Step 90, Train Loss 4.3897\n",
      "Epoch 3/5, Step 100, Train Loss 4.8336\n",
      "Epoch 3/5, Step 110, Train Loss 4.5930\n",
      "Epoch 3/5, Step 120, Train Loss 4.7028\n",
      "Epoch 3/5, Step 130, Train Loss 4.1604\n",
      "Epoch 3/5, Step 140, Train Loss 3.9854\n",
      "Epoch 3/5, Step 150, Train Loss 3.8404\n",
      "Epoch 3/5, Step 160, Train Loss 4.1965\n",
      "Epoch 3/5, Step 170, Train Loss 3.4119\n",
      "Epoch 3/5, Step 180, Train Loss 3.3257\n",
      "Epoch 3/5, Step 190, Train Loss 3.4249\n",
      "Epoch 3/5, Step 200, Train Loss 3.7009\n",
      "Epoch 3/5, Step 210, Train Loss 4.1687\n",
      "Epoch 3/5, Step 220, Train Loss 4.4901\n",
      "Epoch 3/5, Step 230, Train Loss 3.5755\n",
      "Epoch 3/5, Step 240, Train Loss 3.3243\n",
      "Epoch 3/5, Step 250, Train Loss 4.6155\n",
      "Epoch 3/5, Step 260, Train Loss 3.7479\n",
      "Epoch 3/5, Step 270, Train Loss 4.6039\n",
      "Epoch 3/5, Step 280, Train Loss 4.7977\n",
      "Epoch 3/5, Step 290, Train Loss 3.7011\n",
      "Epoch 3/5, Step 300, Train Loss 4.6645\n",
      "Epoch 3/5, Step 310, Train Loss 3.9325\n",
      "Epoch 3/5, Step 320, Train Loss 3.8744\n",
      "Epoch 3/5, Step 330, Train Loss 4.5106\n",
      "Epoch 3/5, Step 340, Train Loss 4.1014\n",
      "Epoch 3/5, Step 350, Train Loss 4.3822\n",
      "Epoch 3/5, Step 360, Train Loss 3.7775\n",
      "Epoch 3/5, Step 370, Train Loss 4.1291\n",
      "Epoch 3/5, Step 380, Train Loss 4.4298\n",
      "Epoch 3/5, Step 390, Train Loss 4.1182\n",
      "Epoch 3/5, Step 400, Train Loss 3.8456\n",
      "Epoch 3/5, Step 410, Train Loss 4.1717\n",
      "Epoch 3/5, Step 420, Train Loss 4.2443\n",
      "Epoch 3/5, Step 430, Train Loss 4.4258\n",
      "Epoch 3/5, Step 440, Train Loss 3.0273\n",
      "Epoch 3/5, Step 450, Train Loss 3.8744\n",
      "Epoch 3/5, Step 460, Train Loss 4.1233\n",
      "Epoch 3/5, Step 470, Train Loss 4.8980\n",
      "Epoch 3/5, Step 480, Train Loss 4.0996\n",
      "Epoch 3/5, Step 490, Train Loss 4.1039\n",
      "Epoch 3/5, Step 500, Train Loss 3.3215\n",
      "Epoch 3/5, Step 510, Train Loss 4.4716\n",
      "Epoch 3/5, Step 520, Train Loss 4.1283\n",
      "Epoch 3/5, Step 530, Train Loss 3.6009\n",
      "Epoch 3/5, Step 540, Train Loss 4.2449\n",
      "Epoch 3/5, Step 550, Train Loss 4.3436\n",
      "Epoch 3/5, Step 560, Train Loss 4.2388\n",
      "Epoch 3/5, Step 570, Train Loss 3.7631\n",
      "Epoch 3/5, Step 580, Train Loss 4.2073\n",
      "Epoch 3/5, Step 590, Train Loss 3.6194\n",
      "Epoch 3/5, Step 600, Train Loss 3.4270\n",
      "Epoch 3/5, Step 610, Train Loss 3.5081\n",
      "Epoch 3/5, Step 620, Train Loss 4.3466\n",
      "Epoch 3/5, Step 630, Train Loss 3.0982\n",
      "Epoch 3/5, Step 640, Train Loss 3.9866\n",
      "Epoch 3/5, Step 650, Train Loss 3.5537\n",
      "Epoch 3/5, Step 660, Train Loss 3.6136\n",
      "Epoch 3/5, Step 670, Train Loss 4.3248\n",
      "Epoch 3/5, Step 680, Train Loss 3.1623\n",
      "Epoch 3/5, Step 690, Train Loss 3.8782\n",
      "Epoch 3/5, Step 700, Train Loss 4.3189\n",
      "Epoch 3/5, Step 710, Train Loss 4.7010\n",
      "Epoch 3/5, Step 720, Train Loss 3.4137\n",
      "Epoch 3/5, Step 730, Train Loss 3.8054\n",
      "Epoch 3/5, Step 740, Train Loss 3.6820\n",
      "Epoch 3/5, Step 750, Train Loss 3.2726\n",
      "Epoch 3/5, Step 760, Train Loss 3.9966\n",
      "Epoch 3/5, Step 770, Train Loss 3.3743\n",
      "Epoch 3/5, Step 780, Train Loss 3.4722\n",
      "Epoch 3/5, Step 790, Train Loss 3.8896\n",
      "Epoch 3/5, Step 800, Train Loss 4.3527\n",
      "Epoch 3/5, Step 810, Train Loss 4.1285\n",
      "Epoch 3/5, Step 820, Train Loss 3.4304\n",
      "Epoch 3/5, Step 830, Train Loss 3.2317\n",
      "Epoch 3/5, Step 840, Train Loss 3.6512\n",
      "Epoch 3/5, Step 850, Train Loss 2.5100\n",
      "Epoch 3/5, Step 860, Train Loss 4.8953\n",
      "Epoch 3/5, Step 870, Train Loss 4.0506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/usr/local/lib/python3.11/dist-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) Nov 13, 2025 8:52:10 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:12 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:14 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:15 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:21 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:22 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:26 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:29 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:29 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:30 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:34 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: 」 (U+300D, decimal: 12301)\n",
      "Nov 13, 2025 8:52:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:36 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:37 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:38 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:39 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:39 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0018 (U+18, decimal: 24)\n",
      "Nov 13, 2025 8:52:41 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:41 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:43 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:49 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:49 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:52 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:54 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:56 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:56 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:56 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:52:58 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:00 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:01 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:01 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:08 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:09 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:09 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:14 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:14 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:15 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:18 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:19 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:21 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:22 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:23 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:24 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:26 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:33 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:36 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:42 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:42 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:43 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:46 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:46 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:47 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:47 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:52 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:54 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:53:58 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:04 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:04 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:04 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0013 (U+13, decimal: 19)\n",
      "Nov 13, 2025 8:54:05 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:07 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0018 (U+18, decimal: 24)\n",
      "Nov 13, 2025 8:54:08 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0018 (U+18, decimal: 24)\n",
      "Nov 13, 2025 8:54:12 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:14 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:15 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:18 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:19 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:19 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:21 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:23 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:24 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:27 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:28 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:30 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:31 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:33 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:33 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:33 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:36 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0018 (U+18, decimal: 24)\n",
      "Nov 13, 2025 8:54:37 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:38 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:38 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:41 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:41 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:43 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:45 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:46 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:47 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:51 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:52 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:53 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:56 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:54:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:00 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0018 (U+18, decimal: 24)\n",
      "Nov 13, 2025 8:55:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:06 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:06 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:08 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:08 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:09 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:10 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:12 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:18 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:19 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:22 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:27 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:27 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:30 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:30 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:30 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:34 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:36 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:36 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:43 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:47 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:48 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:52 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:52 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:52 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:55:58 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:56:01 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:56:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:56:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:56:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:56:04 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:56:06 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 8:56:06 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "[04:16.355 minutes]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 4.434 min\n",
      "Epoch 3 DONE. Val Loss 4.4311, Val Acc 33.63%, SPICE 0.0711\n",
      "Epoch 4/5, Step 10, Train Loss 3.6915\n",
      "Epoch 4/5, Step 20, Train Loss 4.1440\n",
      "Epoch 4/5, Step 30, Train Loss 3.0877\n",
      "Epoch 4/5, Step 40, Train Loss 2.9022\n",
      "Epoch 4/5, Step 50, Train Loss 4.3101\n",
      "Epoch 4/5, Step 60, Train Loss 3.1811\n",
      "Epoch 4/5, Step 70, Train Loss 4.3193\n",
      "Epoch 4/5, Step 80, Train Loss 3.8317\n",
      "Epoch 4/5, Step 90, Train Loss 4.3005\n",
      "Epoch 4/5, Step 100, Train Loss 3.7869\n",
      "Epoch 4/5, Step 110, Train Loss 2.5447\n",
      "Epoch 4/5, Step 120, Train Loss 3.0982\n",
      "Epoch 4/5, Step 130, Train Loss 3.4620\n",
      "Epoch 4/5, Step 140, Train Loss 3.9467\n",
      "Epoch 4/5, Step 150, Train Loss 3.9877\n",
      "Epoch 4/5, Step 160, Train Loss 3.9207\n",
      "Epoch 4/5, Step 170, Train Loss 3.3010\n",
      "Epoch 4/5, Step 180, Train Loss 3.8431\n",
      "Epoch 4/5, Step 190, Train Loss 3.1116\n",
      "Epoch 4/5, Step 200, Train Loss 3.8232\n",
      "Epoch 4/5, Step 210, Train Loss 3.7342\n",
      "Epoch 4/5, Step 220, Train Loss 3.8479\n",
      "Epoch 4/5, Step 230, Train Loss 3.0691\n",
      "Epoch 4/5, Step 240, Train Loss 3.6516\n",
      "Epoch 4/5, Step 250, Train Loss 4.9516\n",
      "Epoch 4/5, Step 260, Train Loss 3.6764\n",
      "Epoch 4/5, Step 270, Train Loss 3.4449\n",
      "Epoch 4/5, Step 280, Train Loss 3.4252\n",
      "Epoch 4/5, Step 290, Train Loss 3.5701\n",
      "Epoch 4/5, Step 300, Train Loss 4.7083\n",
      "Epoch 4/5, Step 310, Train Loss 3.8898\n",
      "Epoch 4/5, Step 320, Train Loss 3.1623\n",
      "Epoch 4/5, Step 330, Train Loss 3.5025\n",
      "Epoch 4/5, Step 340, Train Loss 3.3374\n",
      "Epoch 4/5, Step 350, Train Loss 4.2713\n",
      "Epoch 4/5, Step 360, Train Loss 3.3078\n",
      "Epoch 4/5, Step 370, Train Loss 3.3952\n",
      "Epoch 4/5, Step 380, Train Loss 3.1846\n",
      "Epoch 4/5, Step 390, Train Loss 3.9333\n",
      "Epoch 4/5, Step 400, Train Loss 3.3579\n",
      "Epoch 4/5, Step 410, Train Loss 3.5987\n",
      "Epoch 4/5, Step 420, Train Loss 2.7279\n",
      "Epoch 4/5, Step 430, Train Loss 3.7480\n",
      "Epoch 4/5, Step 440, Train Loss 3.8295\n",
      "Epoch 4/5, Step 450, Train Loss 2.7108\n",
      "Epoch 4/5, Step 460, Train Loss 3.7016\n",
      "Epoch 4/5, Step 470, Train Loss 4.0002\n",
      "Epoch 4/5, Step 480, Train Loss 3.7733\n",
      "Epoch 4/5, Step 490, Train Loss 3.3040\n",
      "Epoch 4/5, Step 500, Train Loss 3.1652\n",
      "Epoch 4/5, Step 510, Train Loss 3.9224\n",
      "Epoch 4/5, Step 520, Train Loss 3.8146\n",
      "Epoch 4/5, Step 530, Train Loss 3.5963\n",
      "Epoch 4/5, Step 540, Train Loss 3.2668\n",
      "Epoch 4/5, Step 550, Train Loss 3.7083\n",
      "Epoch 4/5, Step 560, Train Loss 4.4644\n",
      "Epoch 4/5, Step 570, Train Loss 3.9421\n",
      "Epoch 4/5, Step 580, Train Loss 3.8358\n",
      "Epoch 4/5, Step 590, Train Loss 3.5717\n",
      "Epoch 4/5, Step 600, Train Loss 3.8413\n",
      "Epoch 4/5, Step 610, Train Loss 3.6793\n",
      "Epoch 4/5, Step 620, Train Loss 3.7004\n",
      "Epoch 4/5, Step 630, Train Loss 3.5425\n",
      "Epoch 4/5, Step 640, Train Loss 4.2036\n",
      "Epoch 4/5, Step 650, Train Loss 3.6098\n",
      "Epoch 4/5, Step 660, Train Loss 3.9378\n",
      "Epoch 4/5, Step 670, Train Loss 4.1026\n",
      "Epoch 4/5, Step 680, Train Loss 3.7759\n",
      "Epoch 4/5, Step 690, Train Loss 3.6549\n",
      "Epoch 4/5, Step 700, Train Loss 3.8181\n",
      "Epoch 4/5, Step 710, Train Loss 2.8750\n",
      "Epoch 4/5, Step 720, Train Loss 3.1048\n",
      "Epoch 4/5, Step 730, Train Loss 3.0892\n",
      "Epoch 4/5, Step 740, Train Loss 3.0792\n",
      "Epoch 4/5, Step 750, Train Loss 3.6435\n",
      "Epoch 4/5, Step 760, Train Loss 3.6634\n",
      "Epoch 4/5, Step 770, Train Loss 3.1400\n",
      "Epoch 4/5, Step 780, Train Loss 3.4470\n",
      "Epoch 4/5, Step 790, Train Loss 2.7498\n",
      "Epoch 4/5, Step 800, Train Loss 3.1412\n",
      "Epoch 4/5, Step 810, Train Loss 3.3376\n",
      "Epoch 4/5, Step 820, Train Loss 3.5342\n",
      "Epoch 4/5, Step 830, Train Loss 4.1918\n",
      "Epoch 4/5, Step 840, Train Loss 3.6295\n",
      "Epoch 4/5, Step 850, Train Loss 4.0374\n",
      "Epoch 4/5, Step 860, Train Loss 3.4445\n",
      "Epoch 4/5, Step 870, Train Loss 2.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/usr/local/lib/python3.11/dist-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) Nov 13, 2025 9:17:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:00 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0018 (U+18, decimal: 24)\n",
      "Nov 13, 2025 9:18:06 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:15 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:21 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:40 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:47 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:48 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:48 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0010 (U+10, decimal: 16)\n",
      "Nov 13, 2025 9:18:48 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:18:51 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:05 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:07 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:14 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:29 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:29 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:37 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:39 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:41 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:49 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:52 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:55 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:56 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:56 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:57 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:19:59 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:02 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:06 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:07 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:09 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:15 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:30 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:37 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:39 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:42 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u001f (U+1F, decimal: 31)\n",
      "Nov 13, 2025 9:20:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:20:51 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:21:00 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "[03:32.818 minutes]\n",
      "Error: Could not cache item to /usr/local/lib/python3.11/dist-packages/pycocoevalcap/spice/cache with key:\n",
      "\"a girl character a area teachings ================================================================= =================================================================Normal ================================================================= FriendsVen ================================================================= ================================================================= ================================================================= =================================================================Ven ================================================================= =================================================================Ven =================================================================RomneyVenVen =================================================================Ven ================================================================= Rooms\"\n",
      "Caption may be too long\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 3.701 min\n",
      "Epoch 4 DONE. Val Loss 4.3872, Val Acc 33.45%, SPICE 0.0721\n",
      "Epoch 5/5, Step 10, Train Loss 2.5408\n",
      "Epoch 5/5, Step 20, Train Loss 3.0672\n",
      "Epoch 5/5, Step 30, Train Loss 3.4228\n",
      "Epoch 5/5, Step 40, Train Loss 2.5702\n",
      "Epoch 5/5, Step 50, Train Loss 3.0369\n",
      "Epoch 5/5, Step 60, Train Loss 3.0873\n",
      "Epoch 5/5, Step 70, Train Loss 3.4339\n",
      "Epoch 5/5, Step 80, Train Loss 3.2424\n",
      "Epoch 5/5, Step 90, Train Loss 2.6391\n",
      "Epoch 5/5, Step 100, Train Loss 3.4499\n",
      "Epoch 5/5, Step 110, Train Loss 3.7120\n",
      "Epoch 5/5, Step 120, Train Loss 2.6033\n",
      "Epoch 5/5, Step 130, Train Loss 3.7258\n",
      "Epoch 5/5, Step 140, Train Loss 3.1650\n",
      "Epoch 5/5, Step 150, Train Loss 3.1193\n",
      "Epoch 5/5, Step 160, Train Loss 3.0431\n",
      "Epoch 5/5, Step 170, Train Loss 2.9790\n",
      "Epoch 5/5, Step 180, Train Loss 3.3844\n",
      "Epoch 5/5, Step 190, Train Loss 3.2624\n",
      "Epoch 5/5, Step 200, Train Loss 3.7177\n",
      "Epoch 5/5, Step 210, Train Loss 2.1653\n",
      "Epoch 5/5, Step 220, Train Loss 2.9289\n",
      "Epoch 5/5, Step 230, Train Loss 3.6300\n",
      "Epoch 5/5, Step 240, Train Loss 3.0395\n",
      "Epoch 5/5, Step 250, Train Loss 3.3577\n",
      "Epoch 5/5, Step 260, Train Loss 2.8688\n",
      "Epoch 5/5, Step 270, Train Loss 3.4388\n",
      "Epoch 5/5, Step 280, Train Loss 3.2465\n",
      "Epoch 5/5, Step 290, Train Loss 3.4428\n",
      "Epoch 5/5, Step 300, Train Loss 3.3633\n",
      "Epoch 5/5, Step 310, Train Loss 3.1845\n",
      "Epoch 5/5, Step 320, Train Loss 3.7457\n",
      "Epoch 5/5, Step 330, Train Loss 3.6013\n",
      "Epoch 5/5, Step 340, Train Loss 2.8931\n",
      "Epoch 5/5, Step 350, Train Loss 3.1639\n",
      "Epoch 5/5, Step 360, Train Loss 3.8904\n",
      "Epoch 5/5, Step 370, Train Loss 2.8557\n",
      "Epoch 5/5, Step 380, Train Loss 3.2980\n",
      "Epoch 5/5, Step 390, Train Loss 3.8710\n",
      "Epoch 5/5, Step 400, Train Loss 3.3575\n",
      "Epoch 5/5, Step 410, Train Loss 3.4063\n",
      "Epoch 5/5, Step 420, Train Loss 2.3330\n",
      "Epoch 5/5, Step 430, Train Loss 3.0293\n",
      "Epoch 5/5, Step 440, Train Loss 3.3205\n",
      "Epoch 5/5, Step 450, Train Loss 2.9119\n",
      "Epoch 5/5, Step 460, Train Loss 3.6715\n",
      "Epoch 5/5, Step 470, Train Loss 2.9030\n",
      "Epoch 5/5, Step 480, Train Loss 3.3334\n",
      "Epoch 5/5, Step 490, Train Loss 3.8171\n",
      "Epoch 5/5, Step 500, Train Loss 3.7727\n",
      "Epoch 5/5, Step 510, Train Loss 3.6472\n",
      "Epoch 5/5, Step 520, Train Loss 3.1996\n",
      "Epoch 5/5, Step 530, Train Loss 2.9700\n",
      "Epoch 5/5, Step 540, Train Loss 2.5836\n",
      "Epoch 5/5, Step 550, Train Loss 3.5844\n",
      "Epoch 5/5, Step 560, Train Loss 3.7916\n",
      "Epoch 5/5, Step 570, Train Loss 3.3872\n",
      "Epoch 5/5, Step 580, Train Loss 3.6913\n",
      "Epoch 5/5, Step 590, Train Loss 2.5243\n",
      "Epoch 5/5, Step 600, Train Loss 2.5051\n",
      "Epoch 5/5, Step 610, Train Loss 3.2306\n",
      "Epoch 5/5, Step 620, Train Loss 3.8585\n",
      "Epoch 5/5, Step 630, Train Loss 3.7524\n",
      "Epoch 5/5, Step 640, Train Loss 3.6421\n",
      "Epoch 5/5, Step 650, Train Loss 3.6693\n",
      "Epoch 5/5, Step 660, Train Loss 2.7712\n",
      "Epoch 5/5, Step 670, Train Loss 2.7500\n",
      "Epoch 5/5, Step 680, Train Loss 3.3661\n",
      "Epoch 5/5, Step 690, Train Loss 3.7855\n",
      "Epoch 5/5, Step 700, Train Loss 2.7718\n",
      "Epoch 5/5, Step 710, Train Loss 4.1560\n",
      "Epoch 5/5, Step 720, Train Loss 3.2352\n",
      "Epoch 5/5, Step 730, Train Loss 3.3013\n",
      "Epoch 5/5, Step 740, Train Loss 3.1524\n",
      "Epoch 5/5, Step 750, Train Loss 2.9462\n",
      "Epoch 5/5, Step 760, Train Loss 2.4413\n",
      "Epoch 5/5, Step 770, Train Loss 3.7309\n",
      "Epoch 5/5, Step 780, Train Loss 3.1806\n",
      "Epoch 5/5, Step 790, Train Loss 2.4487\n",
      "Epoch 5/5, Step 800, Train Loss 3.4226\n",
      "Epoch 5/5, Step 810, Train Loss 3.5036\n",
      "Epoch 5/5, Step 820, Train Loss 3.1684\n",
      "Epoch 5/5, Step 830, Train Loss 3.5870\n",
      "Epoch 5/5, Step 840, Train Loss 3.1378\n",
      "Epoch 5/5, Step 850, Train Loss 3.9641\n",
      "Epoch 5/5, Step 860, Train Loss 4.0891\n",
      "Epoch 5/5, Step 870, Train Loss 3.3303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/usr/local/lib/python3.11/dist-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) Nov 13, 2025 9:42:26 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:27 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:34 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:42 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:46 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:53 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:56 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:42:59 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:03 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0010 (U+10, decimal: 16)\n",
      "Nov 13, 2025 9:43:05 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:06 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:08 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:10 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:11 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:12 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:13 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:20 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:22 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:25 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:42 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:43 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:43:52 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:16 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:18 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:24 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:28 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:30 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:33 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:44:46 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:01 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:07 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:17 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: \u0010 (U+10, decimal: 16)\n",
      "Nov 13, 2025 9:45:26 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:27 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:33 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:35 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:39 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:44 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:45 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:48 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:48 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:50 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:55 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "Nov 13, 2025 9:45:56 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: � (U+FFFD, decimal: 65533)\n",
      "[03:50.410 minutes]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 3.989 min\n",
      "Epoch 5 DONE. Val Loss 4.3587, Val Acc 34.41%, SPICE 0.0776\n",
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import tiktoken\n",
    "\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# ========================\n",
    "# CONFIGURATION\n",
    "# ========================\n",
    "DATA_ROOT      = r\"/kaggle/input/dataforvtt/data\"         # train data root\n",
    "# DATA_ROOT      = r\"\"         # train data root\n",
    "CLIPS_DIR      = os.path.join(DATA_ROOT, \"clips\")\n",
    "CAPTIONS_FILE  = os.path.join(DATA_ROOT, \"captions.json\")\n",
    "\n",
    "VAL_ROOT       = \"/kaggle/input/data-val/data_val\"     # validation data root\n",
    "VAL_CLIPS_DIR  = os.path.join(VAL_ROOT, \"clips\")\n",
    "VAL_CAPTIONS   = os.path.join(VAL_ROOT, \"captions.json\")\n",
    "\n",
    "EPOCHS      = 5\n",
    "BATCH_SIZE  = 8\n",
    "LR          = 1e-4\n",
    "EMBED_DIM   = 512\n",
    "MAX_LEN     = 32\n",
    "ALPHA       = 0.1  # contrastive loss weight\n",
    "\n",
    "# ========================\n",
    "# TOKENIZER\n",
    "# ========================\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "BOS_ID, EOS_ID, PAD_ID = 50256, 50256, 0\n",
    "\n",
    "# ========================\n",
    "# DATASET\n",
    "# ========================\n",
    "class MSRVTTClipsDataset(Dataset):\n",
    "    def __init__(self, clips_dir, captions_file, max_len=MAX_LEN):\n",
    "        self.clips_dir = clips_dir\n",
    "        with open(captions_file, \"r\") as f:\n",
    "            self.captions = json.load(f)\n",
    "        self.video_ids = sorted(list(self.captions.keys()))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        ids = [BOS_ID] + tokenizer.encode(text) + [EOS_ID]\n",
    "        arr = np.full(self.max_len, PAD_ID, dtype=np.int64)\n",
    "        arr[:min(len(ids), self.max_len)] = ids[:self.max_len]\n",
    "        return torch.tensor(arr, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.video_ids[idx]\n",
    "        clip = np.load(os.path.join(self.clips_dir, vid, \"clip.npy\")).astype(np.float32)\n",
    "        if clip.max() > 1.5:\n",
    "            clip /= 255.0\n",
    "        cap_text = self.captions[vid][0]\n",
    "        cap_ids = self._tokenize(cap_text)\n",
    "        return torch.from_numpy(clip), cap_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "\n",
    "# ========================\n",
    "# C3D Backbone\n",
    "# ========================\n",
    "class C3DBackbone(nn.Module):\n",
    "    def __init__(self, out_dim=4096):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(3,64,3,padding=1), nn.ReLU(), nn.MaxPool3d(2,2),\n",
    "            nn.Conv3d(64,128,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool3d((1,1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(128, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ========================\n",
    "# Encoder\n",
    "# ========================\n",
    "class EncoderResNetAllFramesC3D(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM, c3d_out=4096):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.res_proj = nn.Linear(2048, embed_dim)\n",
    "        self.c3d = C3DBackbone(out_dim=c3d_out)\n",
    "        self.c3d_proj = nn.Linear(c3d_out, embed_dim)\n",
    "        self.fuse = nn.Linear(embed_dim*2, embed_dim)\n",
    "        self.bn = nn.BatchNorm1d(embed_dim)\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.c3d.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, clip_16):\n",
    "        B = clip_16.size(0)\n",
    "        frames = clip_16.permute(0,2,1,3,4).reshape(B*16,3,112,112)\n",
    "        frames = F.interpolate(frames, size=(224,224), mode=\"bilinear\")\n",
    "        frames = (frames - self.mean) / self.std\n",
    "        with torch.no_grad():\n",
    "            r = self.resnet(frames)\n",
    "        r = self.res_proj(r.view(B*16, -1)).view(B, 16, -1).mean(1)\n",
    "        with torch.no_grad():\n",
    "            c = self.c3d(clip_16)\n",
    "        c = self.c3d_proj(c)\n",
    "        f = self.fuse(torch.cat([r, c], 1))\n",
    "        return self.bn(f)\n",
    "\n",
    "# ========================\n",
    "# Decoder\n",
    "# ========================\n",
    "class GPTStyleDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM, vocab_size=VOCAB_SIZE,\n",
    "                 n_heads=8, n_layers=4, ff_dim=2048, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "        layer = nn.TransformerDecoderLayer(embed_dim, n_heads, ff_dim, activation=\"gelu\")\n",
    "        self.decoder = nn.TransformerDecoder(layer, n_layers)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, vid_emb, token_ids):\n",
    "        B, T = token_ids.shape\n",
    "        pos = torch.arange(T, device=token_ids.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.tok_emb(token_ids) + self.pos_emb(pos)\n",
    "        mem = vid_emb.unsqueeze(1).expand(B, T, -1)\n",
    "        x, mem = x.transpose(0, 1), mem.transpose(0, 1)\n",
    "        mask = torch.triu(torch.ones(T, T, device=token_ids.device) * float(\"-inf\"), 1)\n",
    "        y = self.decoder(x, mem, tgt_mask=mask)\n",
    "        y = y.transpose(0, 1)\n",
    "        pooled = y.mean(dim=1)\n",
    "        logits = self.lm_head(y)\n",
    "        return logits, pooled\n",
    "\n",
    "# ========================\n",
    "# Contrastive Loss\n",
    "# ========================\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, video_embed, caption_embed):\n",
    "        video_norm = F.normalize(video_embed, dim=-1)\n",
    "        caption_norm = F.normalize(caption_embed, dim=-1)\n",
    "        logits = torch.matmul(video_norm, caption_norm.T) / self.temperature\n",
    "        labels = torch.arange(video_embed.size(0), device=video_embed.device)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "# ========================\n",
    "# Greedy Decode\n",
    "# ========================\n",
    "def greedy_decode(enc, dec, clip_path, max_len=MAX_LEN, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    clip = np.load(clip_path).astype(np.float32)\n",
    "    if clip.max() > 1.5:\n",
    "        clip /= 255.0\n",
    "    clip = torch.from_numpy(clip).unsqueeze(0).to(device).float()\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = enc(clip)\n",
    "        seq = [BOS_ID]\n",
    "        for _ in range(max_len - 1):\n",
    "            inp = torch.tensor(seq, device=device).unsqueeze(0)\n",
    "            logits, _ = dec(emb, inp)\n",
    "            nxt = int(logits[0, -1].argmax())\n",
    "            seq.append(nxt)\n",
    "            if nxt == EOS_ID:\n",
    "                break\n",
    "    return tokenizer.decode([i for i in seq if i not in (BOS_ID, EOS_ID, PAD_ID)])\n",
    "\n",
    "# ========================\n",
    "# Evaluate with SPICE\n",
    "# ========================\n",
    "def evaluate(enc, dec, dataloader, crit, contrastive_crit, alpha, device):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    spice = Spice()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    correct = 0\n",
    "    all_gen = []\n",
    "    all_refs = []\n",
    "    with torch.no_grad():\n",
    "        for clip, cap in dataloader:\n",
    "            clip, cap = clip.to(device).float(), cap.to(device)\n",
    "            vid_emb = enc(clip)\n",
    "            logits, cap_emb = dec(vid_emb, cap[:, :-1])\n",
    "            target = cap[:, 1:]\n",
    "            ce_loss = crit(logits.reshape(-1, logits.size(-1)), target.reshape(-1))\n",
    "            contrast_loss = contrastive_crit(vid_emb, cap_emb)\n",
    "            loss = ce_loss + alpha * contrast_loss\n",
    "            total_loss += loss.item() * clip.size(0)\n",
    "            preds = logits.argmax(-1)\n",
    "            mask = target != PAD_ID\n",
    "            correct += (preds == target).masked_select(mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "            # prepare for SPICE\n",
    "            for b in range(clip.size(0)):\n",
    "                img_id = len(all_gen)\n",
    "                pred_ids = preds[b].tolist()\n",
    "                pred_caption = tokenizer.decode([i for i in pred_ids if i not in (BOS_ID, EOS_ID, PAD_ID)])\n",
    "                true_ids = target[b].tolist()\n",
    "                true_caption = tokenizer.decode([i for i in true_ids if i not in (BOS_ID, EOS_ID, PAD_ID)])\n",
    "                all_gen.append((img_id, pred_caption))\n",
    "                all_refs.append((img_id, [true_caption]))\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    acc = correct / total_tokens if total_tokens > 0 else 0\n",
    "    # Convert lists into dictionaries\n",
    "    gen_dict = {img_id: [caption] for img_id, caption in all_gen}\n",
    "    ref_dict = {img_id: refs for img_id, refs in all_refs}\n",
    "    spice_score, _ = spice.compute_score(ref_dict, gen_dict)\n",
    "    return avg_loss, acc, spice_score\n",
    "\n",
    "# ========================\n",
    "# Train Model\n",
    "# ========================\n",
    "def train_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    train_ds = MSRVTTClipsDataset(CLIPS_DIR, CAPTIONS_FILE, max_len=MAX_LEN)\n",
    "    val_ds = MSRVTTClipsDataset(VAL_CLIPS_DIR, VAL_CAPTIONS, max_len=MAX_LEN)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    enc = EncoderResNetAllFramesC3D(EMBED_DIM).to(device)\n",
    "    dec = GPTStyleDecoder(embed_dim=EMBED_DIM, max_len=MAX_LEN).to(device)\n",
    "\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "    contrastive_crit = ContrastiveLoss()\n",
    "    params = list(dec.parameters()) + list(enc.res_proj.parameters()) + \\\n",
    "             list(enc.c3d_proj.parameters()) + list(enc.fuse.parameters()) + list(enc.bn.parameters())\n",
    "    opt = optim.Adam(params, lr=LR)\n",
    "\n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        enc.train()\n",
    "        dec.train()\n",
    "        for step, (clip, cap) in enumerate(train_dl, 1):\n",
    "            clip, cap = clip.to(device).float(), cap.to(device)\n",
    "            vid_emb = enc(clip)\n",
    "            logits, cap_emb = dec(vid_emb, cap[:, :-1])\n",
    "            target = cap[:, 1:]\n",
    "            ce_loss = crit(logits.reshape(-1, logits.size(-1)), target.reshape(-1))\n",
    "            contrast_loss = contrastive_crit(vid_emb, cap_emb)\n",
    "            loss = ce_loss + ALPHA * contrast_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            opt.step()\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {ep}/{EPOCHS}, Step {step}, Train Loss {loss.item():.4f}\")\n",
    "\n",
    "        val_loss, val_acc, spice_score = evaluate(enc, dec, val_dl, crit, contrastive_crit, ALPHA, device)\n",
    "        print(f\"Epoch {ep} DONE. Val Loss {val_loss:.4f}, Val Acc {val_acc*100:.2f}%, SPICE {spice_score:.4f}\")\n",
    "\n",
    "    torch.save(enc.state_dict(), \"/kaggle/working/encoder.pth\")\n",
    "    torch.save(dec.state_dict(), \"/kaggle/working/decoder.pth\")\n",
    "    print(\"Models saved successfully.\")\n",
    "    return enc, dec\n",
    "\n",
    "# ========================\n",
    "# Main\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    enc, dec = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T19:37:24.552869Z",
     "iopub.status.busy": "2025-11-13T19:37:24.552591Z",
     "iopub.status.idle": "2025-11-13T19:37:27.516628Z",
     "shell.execute_reply": "2025-11-13T19:37:27.515868Z",
     "shell.execute_reply.started": "2025-11-13T19:37:24.552834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T19:49:42.926443Z",
     "iopub.status.busy": "2025-09-10T19:49:42.926205Z",
     "iopub.status.idle": "2025-09-10T19:49:42.940022Z",
     "shell.execute_reply": "2025-09-10T19:49:42.939093Z",
     "shell.execute_reply.started": "2025-09-10T19:49:42.926418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Create clickable links for download\n",
    "display(FileLink(\"/kaggle/working/encoder.pth\"))\n",
    "display(FileLink(\"/kaggle/working/decoder.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T19:50:31.824967Z",
     "iopub.status.busy": "2025-09-10T19:50:31.824648Z",
     "iopub.status.idle": "2025-09-10T19:50:31.831865Z",
     "shell.execute_reply": "2025-09-10T19:50:31.830331Z",
     "shell.execute_reply.started": "2025-09-10T19:50:31.824941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T21:46:10.715969Z",
     "iopub.status.busy": "2025-11-13T21:46:10.715745Z",
     "iopub.status.idle": "2025-11-13T21:46:31.707589Z",
     "shell.execute_reply": "2025-11-13T21:46:31.706820Z",
     "shell.execute_reply.started": "2025-11-13T21:46:10.715951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Captions:\n",
      "Video ID: video2789\n",
      "Caption: a man is giving a speech\n",
      "\n",
      "Video ID: video3483\n",
      "Caption: a band is performing a song on stage\n",
      "\n",
      "Video ID: video4184\n",
      "Caption: a woman is riding a bike on a road\n",
      "\n",
      "Video ID: video3654\n",
      "Caption: a man is talking about a man\n",
      "\n",
      "Video ID: video2795\n",
      "Caption: a clip from a movie is playing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "def predict_captions(enc, dec, clips_dir, num_samples=5, max_len=MAX_LEN, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Get list of video clip files\n",
    "    video_files = [f for f in os.listdir(clips_dir) if os.path.exists(os.path.join(clips_dir, f, \"clip.npy\"))]\n",
    "    \n",
    "    # Sample random videos\n",
    "    sample_vids = sample(video_files, min(num_samples, len(video_files)))\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for vid in sample_vids:\n",
    "            # Load and preprocess clip\n",
    "            clip_path = os.path.join(clips_dir, vid, \"clip.npy\")\n",
    "            clip = np.load(clip_path).astype(np.float32)\n",
    "            if clip.max() > 1.5:\n",
    "                clip /= 255.0\n",
    "            clip = torch.from_numpy(clip).unsqueeze(0).to(device).float()\n",
    "            \n",
    "            # Generate caption\n",
    "            caption = greedy_decode(enc, dec, clip_path, max_len=max_len, device=device)\n",
    "            \n",
    "            predictions.append({\n",
    "                'video_id': vid,\n",
    "                'predicted_caption': caption\n",
    "            })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Assuming enc and dec are already loaded from train_model()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load saved models if not already loaded\n",
    "    enc = EncoderResNetAllFramesC3D(embed_dim=EMBED_DIM).to(device)\n",
    "    dec = GPTStyleDecoder(embed_dim=EMBED_DIM, max_len=MAX_LEN).to(device)\n",
    "    enc.load_state_dict(torch.load(\"/kaggle/working/encoder.pth\", map_location=device))\n",
    "    dec.load_state_dict(torch.load(\"/kaggle/working/decoder.pth\", map_location=device))\n",
    "    \n",
    "    # Predict captions for 5 random training videos\n",
    "    predictions = predict_captions(enc, dec, CLIPS_DIR, num_samples=5, max_len=MAX_LEN, device=device)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nPredicted Captions:\")\n",
    "    for pred in predictions:\n",
    "        print(f\"Video ID: {pred['video_id']}\")\n",
    "        print(f\"Caption: {pred['predicted_caption']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8210509,
     "sourceId": 12972301,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8210596,
     "sourceId": 12972424,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
